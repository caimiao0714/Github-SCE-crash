---
title: Supplemental Materials 
subtitle: |
  | The association between crashes and safety-critical events: 
  | synthesized evidence from crash reports and naturalistic driving data among commercial truck drivers
#author: Author names anonymized for peer-review
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: yes
    number_sections: yes
    fig_caption: yes
vignette: 
  \VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
params:
  EVAL: !r identical(Sys.getenv("NOT_CRAN"), "true")
bibliography: reference.bib
---


<style type="text/css">
h1.title {
  font-size: 20px;
}
h1 { /* Header 1 */
  font-size: 20px;
}
h2 { /* Header 2 */
    font-size: 15px;
}
</style>



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.width = 7, 
                      fig.height = 7*0.618, 
                      fig.align = "center", 
                      fig.retina = 2, 
                      cache = F)
```

[![DOI](https://zenodo.org/badge/215154957.svg)](https://zenodo.org/badge/latestdoi/215154957)

This website serves as the supplementary materials for the manuscript *The association between crashes and safety-critical events: synthesized evidence from crash reports and naturalistic driving data among commercial truck drivers* by Miao Cai, Mohammad Ali Alamdar Yazdi, Amir Mehdizadeh, Qiong Hu, Alexander Vinel, Karen Davis, Fadel Megahed, Hong Xian, and Steven Rigdon. It is under 1st revision in [Transportation Research Part C: Emerging Technologies](https://www.journals.elsevier.com/transportation-research-part-c-emerging-technologies).

The website is hold on [a GitHub repository](https://github.com/caimiao0714/Github-SCE-crash). The data is accessible at [the data folder](https://github.com/caimiao0714/Github-SCE-crash/tree/master/data). The Rmarkdown file that includes all the code to reproduce this website is accessible [here](https://github.com/caimiao0714/Github-SCE-crash/blob/master/index.Rmd).

Ping data and trip aggregation
==============================
Ping data demonstration
-----------------------
```{r originping}
pacman::p_load(data.table, dplyr, ggplot2, lubridate, kableExtra)
d = fread("data/sample_ping.csv") %>% 
  .[,ping_time := ymd_hms(ping_time)]

knitr::kable(d[1:10,], align = "c", caption = "Sample ping data")
```

The above table shows a sample of the original data. It includes the exact date and time of the record (year, month, day, hour, minute, and second), latitude and longitude (specific to five decimal places), speed, and drivers' anonymized unique ID.

Aggregating ping data into trips
--------------------------------
The self-defined function `segment_0()` below is used to separate the original ping data into trips according to a threshold value.

```{r}
segment_0 = function(speed, threshold, time_diff) {
  speed1 = speed
  speed[time_diff >= threshold] <- 0
  r1 = rle(speed != 0)
  r1$values <- replicate(length(r1$values), 1)
  r1$values <- cumsum(r1$values)
  order_tmp <- inverse.rle(r1)
  dat_tmp1 <- data.table::data.table(speed, order_tmp, time_diff)
  dat_tmp2 <- dat_tmp1[,.(sumdiff = sum(time_diff)), by = order_tmp]
  r2 = rle(speed != 0); first_rle = r2$values[1]
  r2$values[r2$values == 0 & dat_tmp2$sumdiff < threshold] <- TRUE
  r2$values[1] = first_rle
  r2 <- rle(inverse.rle(r2))
  r2$values[r2$values] = cumsum(r2$values[r2$values])
  id = inverse.rle(r2)
  jump_speed = which(id == 0 & speed1 != 0)
  id[jump_speed] = id[jump_speed + 1]
  return(id)
}
```

The code block below shows R code to separate the original ping data into different trips by adding a `trip_id` column. Here we set the threshold value as 30 minutes, which means that the ping data will be separated into different trips when the vehicle is not moving (the speed of the ping equals zero) for more than 30 minutes. The pings that has `trip_id` of 0 are stopping pings.

```{r pingID}
d_id = d %>% 
  .[,diff := as.integer(difftime(ping_time, shift(ping_time, type = "lag", fill = 0), 
                                 units = "mins")), driver] %>%
  .[,diff := {diff[1] = 0L; diff}, driver] %>%
  .[,trip_id := segment_0(speed = speed, threshold = 30, time_diff = diff), driver]

knitr::kable(d_id[1:10,], align = "c", caption = "Sample ping data with time difference and trip ID")
```

The table below is the aggregated trips data, including start time, end time, trip length, and the median trip time. The ping table and trips table can be merged into one table using driver ID and trip ID.

```{r trips}
d_trip = d %>%
  .[trip_id != 0,] %>% 
  .[,.(start_time = ping_time[1], end_time = ping_time[.N]), .(driver, trip_id)] %>% 
  .[,trip_length := round(as.numeric(difftime(end_time, start_time, units = "mins")), 2)] %>% 
  .[,trip_median := start_time + 60*trip_length/2]

knitr::kable(d_trip, align = "c", caption = "Aggregated trips from sample ping")
```

Ping and trip data visualization
--------------------------------

```{r}
p2 = d %>% 
  .[,p_color := case_when(speed >= 50 ~ ">=50 MPH", 
                             speed >= 25 & speed < 50 ~ "(25, 50] MPH", 
                             speed >   0 & speed < 25 ~ "(0, 25] MPH", 
                             speed == 0 ~ "0 MPH")] %>% 
  .[,p_color := factor(p_color, levels = c("0 MPH", "(0, 25] MPH", "(25, 50] MPH", ">=50 MPH"))] %>% 
  ggplot(aes(ping_time, speed)) + 
  geom_point(aes(color = p_color)) + 
  scale_colour_manual(name = "speed category", values = c("#636363", "#31a354", "#fb6a4a", "#a50f15")) + 
  geom_line() + theme_bw() + 
  geom_segment(data = d_trip, aes(x = start_time, xend = end_time, y = -3, yend = -3),
               arrow = arrow(length = unit(.2, "cm")), lineend = 'butt', size = .8, color = "#7b3294") + 
  geom_text(data = d_trip, aes(x = trip_median, y = rep(-4.8, nrow(d_trip)),
                             label = paste(rep("Trip", nrow(d_trip)), 1:nrow(d_trip), " ")),
            color = "#7b3294", size = 3) + 
  labs(x = "date and time of ping", y = "ping speed (MPH)") + 
  scale_y_continuous(breaks = c(0, 25, 50), limits = c(-5, 70)) + 
  theme(legend.justification = c(1, 1), legend.position = c(0.97, 1),
        legend.background = element_rect(fill=alpha('white', 0.1)),
        legend.direction="horizontal", 
        panel.grid.major.x = element_blank(), panel.grid.minor = element_blank())
p2
```

The figure above shows the data aggregation process. The x-axis shows the data and time of pings, and the y-axis presents the speed of the ping (miles per hour, MPH). Each point represented a ping at that date and time, with different colors indicating the real-time speed category. Whenever the truck stopped (the grey points) for at least 30 minutes, the pings were separated into different trips, indicated by the purple arrows in the bottom (Trip 1, Trip 2, $\ldots$, Trip 6). The trip time was then calculated by taking the difference between the trip end time and start time. 

Statistical modeling
====================
Aggregated driver-level data
----------------------------
The table below demonstrates the data for Bayesian Negative Binomial (NB) regression models. The definition of the safety-critical events variables is:

- `sce_CM`: the number of collision mitigation,
- `sce_HB`: the number of hard brakes,
- `sce_HW`: the number of headways,
- `sce_RS`: the number of rolling stability,
- `sce_N`: the number of any type of SCEs, so it should be the sum of `sce_CM`, `sce_HB`, `sce_HW`, and `sce_RS`.

```{r}
pacman::p_load(rstanarm, broom)
stat = fread("data/Bayesian_NB_data.csv") %>% 
  .[order(driver),]
knitr::kable(stat[1:5,], align = "c", 
             caption = "A demonstration for aggregated commercial truck driver data") %>% 
  kable_styling() %>%
  scroll_box(width = "100%")
```

Bayesian NB regression using `rstanarm`
---------------------------------------
The code below shows the code for Bayesian NB regression models. For demonstration purpose, we only use the first 1,000 observations of the data, 1 Markov chain with 1,000 iterations and the first 500 of them are warm-up iterations.

```{r}
stat = stat %>% 
  mutate(sce_CM = sce_CM*10000/distance,
         sce_HB = sce_HB*10000/distance,
         sce_HW = sce_HW*10000/distance,
         sce_RS = sce_RS*10000/distance,
         sce_N = sce_N*10000/distance )

fit <- stan_glm(n_crash ~ sce_N + age + ping_speed + gender + bus_unit + d_type,
                    offset = log(distance/1000),
                    data = stat[1:1000,], family = neg_binomial_2, 
                    prior = normal(0,10), 
                    prior_intercept = normal(0,10), QR = TRUE,
                    iter = 1000, chains = 1, cores = 1, seed = 123)
```

```{r}
broom::tidy(fit, intervals = TRUE, prob = 0.95) %>% 
  mutate(estimate = exp(estimate),
         lower = exp(lower),
         upper = exp(upper)) %>% 
  select(term, IRR = estimate, `95% CI left` = lower, `95% CI right` = upper) %>% 
  knitr::kable(align = "c", 
               caption = "Posterior estimates of Bayesian NB model.")
```

The table above shows the incident rate ratios of the Bayesian NB regression, as well as their 95% credible intervals. The Bayesian interpretation is that there is 95% chance that the IRRs were within these intervals.

Model comparison and diagnostics using `loo`
--------------------------------------------

```{r}
prop_zero <- function(y) mean(y == 0)
pp_check(fit, plotfun = "stat", stat = "prop_zero")
```

The above figure shows posterior predictive checks, which is a measure of the prediction accuracy. It compares the observed data to 100 replicated datasets generated from the posterior parameters distributions. For each simulated dataset, the proportion of zero crashes was computed, and the blue histograms shows the simulated distribution of the proportions. The black solid vertical lines are the observed proportion of zero crashes in observed data. When the observed proportion (black solid line) is near the center of the plot, it demonstrates good model fit.

```{r}
fit_loo = loo(fit)
fit_loo
```

The above block shows the expected log predicted density (`elpd_loo`), estimate number of parameters (`p_loo`), and the LOO Information Criterion (`looic`) for a new dataset from Pareto smoothed importance-sampling leave-one-out (PSIS-LOO) cross-validation (CV). They can be used to check the goodness-of-fit of and compare different models [@vehtari2017practical; @vehtari2015pareto]. In a well-specified model, `p_loo` should be close to the total number of parameters. Similar to the Akaike information criterion (AIC) and Bayesian information criterion (BIC), the `looic` in the output can be used to compare different models, with lower values indicating better models. 

```{r}
fit_new <- stan_glm(n_crash ~ sce_N + age + ping_speed,
                    offset = log(distance/1000),
                    data = stat[1:1000,], family = neg_binomial_2, 
                    prior = normal(0,10), 
                    prior_intercept = normal(0,10), QR = TRUE,
                    iter = 1000, chains = 1, cores = 1, seed = 123)
fit_new_loo = loo(fit_new)
```

```{r}
loo::compare(fit_loo, fit_new_loo)
```

With two model fits `fit` and `fit_new` above, researchers can compare the model fit using `compare()` from the `loo` package, as shown above. It compares the expected predictive accuracy by the difference in `elpd_loo`, with positive difference `elpd_diff` suggesting the second model while negative difference favoring the first model.

Session information {-}
===================
The R session information when building this website is shown below:

```{r}
sessionInfo()
```


Reference {-}
=========